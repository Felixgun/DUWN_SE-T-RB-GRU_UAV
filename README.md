# DUWN_SE-T-RB-GRU_UAV  
*A Transformer-SE-Residual-BiGRU Model for Gesture Recognition via Distributed UWB Network to Control UAVs*

![Model Architecture](./Tr-SE-Res-Bi-GRU%20Diagram1.png)  
*Figure 1: Model Architecture*

---

## üìò Overview

**DUWN_SE-T-RB-GRU_UAV** is a deep learning framework designed to perform **gesture recognition** using signals captured by a **Distributed Ultra-Wideband (UWB) Network**, enabling **real-time UAV control**. The system leverages a hybrid architecture combining **Transformer**, **Squeeze-and-Excitation (SE)** modules, **Residual connections**, and a **Bidirectional GRU (BiGRU)** to enhance temporal and spatial signal understanding.

It supports:
- Model **training and testing**
- Real-time **inference on PC**
- Optimized **inference on Jetson Xavier NX**
- Full UAV flight control via **Jetson-based inference**

---

## üß† Acronym Breakdown

- **DUWN**: Distributed Ultra-Wideband Network  
- **SE-T**: Transformer with Squeeze-and-Excitation Block
- **R**: Residual  
- **B**: Bidirectional
- **GRU**: Gated Recurrent Unit  
- **UAV**: Unmanned Aerial Vehicle

---

## üìÇ Repository Contents

| Folder | Description |
|--------|-------------|
| `res-bi-gru-ablation-multisequence.py` | Scripts for model training and evaluation |
| `inference2.5.py` | Inference code runnable on standard PC |
| `inference-nocamera3-jetson.py` | Inference code optimized for Jetson NX |
| `fly_NX3.py` | Jetson NX code integrated with UAV flight commands |
| `dataset3.zip` | Training datasets |
| `test1.zip` | Testing datasets |

---

## üîß Installation & Setup

**Requirements:**
- Python ‚â• 3.8
- tensorflow = 2.10.1
- NumPy, OpenCV, etc.
- Jetson Xavier NX with JetPack SDK (for deployment)




## üß™ Results

![Flight Result](./test14des1-2.png)  
*Figure 2: UAV flight path based on gesture input*

---

## üñºÔ∏è System Architecture

![System Setup](./Picture1.png)  
*Figure 3: Distributed UWB Network and UAV control setup*

---

## üìä Dataset

Both training and testing datasets are included :
- Collected from multiple UWB anchors and a wearable tag  
- Includes labeled gesture sequences  
- Preprocessed for training
- `inference2.5.py` is used for data collection

---

## üî¨ Model Details

- **Transformer**: Extracts global dependencies from sequential UWB features  
- **SE blocks**: Recalibrate feature maps based on inter-channel relations  
- **Residual blocks**: Ensure deeper network training stability  
- **BiGRU**: Captures bidirectional temporal dynamics

---

## üìö References & Related Work

- TBA

---

## üë§ Author

**Your Name**  
- GitHub: [@Felixgun](https://github.com/Felixgun)  
- Email: felix.iniemail@yahoo.com  
- LinkedIn: [https://linkedin.com/in/felixg26/](https://linkedin.com/in/felixg26/)

---



## üìö Citation & Academic Use

This project was developed as part of my Master's thesis and is currently associated with a research paper that is **under review**. The repository is shared for academic preview and reference purposes only.

üîí Until the paper is formally published, please **do not reuse, distribute, or build upon this code or content** without prior permission.

If you are interested in citing this work, collaborating, or have related inquiries, feel free to reach out:

üì¨ Contact: felix.iniemail@yahoo.com  

A proper citation block (BibTeX) will be added once the publication is finalized.

---

## üóÇÔ∏è Legal & Licensing Notice

This repository does not yet include a formal open-source license.  
As such, all rights are reserved by the author unless explicitly stated otherwise.

> ‚ö†Ô∏è You may browse the code and materials, but reuse or redistribution is **not permitted** at this time.
